# âš™ï¸ DocumentaciÃ³n del Pipeline de TransformaciÃ³n e Ingesta Silver

Este documento describe el flujo de transformaciÃ³n desde la capa **Bronce** a **Silver** utilizando Dagster, Polars y DuckDB. Incluye la lÃ³gica modular implementada con `@op`, su orquestaciÃ³n con `@job` y su ejecuciÃ³n automatizada con `@schedule`.

---

## ğŸ¯ Objetivo

Transformar los datos crudos y no validados (capa Bronce) en datos **estructurados, limpios y listos para anÃ¡lisis o modelado** (capa Silver), manteniendo trazabilidad, consistencia de tipos y cobertura de sensores.

---

## ğŸ§± Estructura General del Pipeline

```plaintext
load_bronze_data
      â†“
prepare_equipos_silver
      â†“
filter_valid_sensors
      â†“
interpolate_sensors
      â†“
generate_sensor_catalog
      â†“
prepare_eventos_silver
      â†“
enrich_eventos_with_equipo
      â†“
generate_pozos_summary
      â†“
save_all_silver_outputs
      â†“
register_silver_tables
```
---

## âš™ï¸ Componentes TÃ©cnicos
### ğŸ§© Orquestador: Dagster
* Todos los pasos del flujo estÃ¡n implementados como @op.
* El @job transform_all_silver_op_based los encadena de manera secuencial.
* Se define un @schedule diario a las 6:30am en zona horaria America/Mexico_City.

### ğŸ“¦ Base de datos: DuckDB
* Los archivos .parquet generados en Silver son registrados como tablas externas.
* Las tablas Silver viven en el esquema silver.* dentro del archivo warehouse.duckdb.

### ğŸ§  LibrerÃ­as usadas
* polars para transformaciÃ³n rÃ¡pida y eficiente.
* duckdb como motor analÃ­tico local.
* dagster para orquestaciÃ³n de pipelines.

---

## ğŸ§¾ DescripciÃ³n de los pasos

| Etapa (`@op`)                | DescripciÃ³n                                                                    |
|----------------------------------------------------------------------------                                   |
| `load_bronze_data`           | Carga sensores, equipos y eventos desde Bronce en DuckDB.                      |
| `prepare_equipos_silver`     | Asigna estado `'activo'` o `'inactivo'` a cada equipo segÃºn fecha de salida.   |
| `filter_valid_sensors`       | Conserva solo lecturas dentro del rango operativo del equipo.                  |
| `interpolate_sensors`        | Resamplea a cada 10 minutos e interpola valores numÃ©ricos por equipo.          |
| `generate_sensor_catalog`    | Crea una tabla de sensores disponibles por pozo-equipo.                        |
| `prepare_eventos_silver`     | Renombra columnas para alinearse con el esquema Silver.                        |
| `enrich_eventos_with_equipo` | Asocia eventos con el nÃºmero de equipo correspondiente (basado en fechas).     |
| `generate_pozos_summary`     | Genera resumen por pozo: cantidad de equipos, estado, Ãºltima entrada.          |
| `save_all_silver_outputs`    | Guarda archivos `.parquet` en la carpeta `data/lake/silver/`.                  |
| `register_silver_tables`     | Registra las tablas como vistas externas en DuckDB.                            |

---

## ğŸ—‚ï¸ UbicaciÃ³n de archivos generados
Los resultados de la transformaciÃ³n se guardan como .parquet en:

```plaintext
data/lake/silver/
â”œâ”€â”€ lecturas_silver/
â”œâ”€â”€ sensor_coverage_silver/
â”œâ”€â”€ equipos_silver/
â”œâ”€â”€ eventos_silver/
â””â”€â”€ pozos_silver/
```

---

## ğŸ”„ AutomatizaciÃ³n
El flujo transform_all_silver_op_based se ejecuta automÃ¡ticamente gracias al siguiente schedule:

```python
ScheduleDefinition(
    job=transform_all_silver_op_based,
    cron_schedule="30 6 * * *",
    execution_timezone="America/Mexico_City",
    name="daily_transform_silver_schedule"
)
```

---

## ğŸ“Œ Notas importantes
* Todas las fechas trabajan en datetime[Î¼s, America/Mexico_City] para coherencia con sensores.
* Los .parquet son vÃ¡lidos como fuente directa para motores analÃ­ticos y ML.
* El @op register_silver_tables se ejecuta despuÃ©s de guardar los archivos, para evitar errores de sincronizaciÃ³n con DuckDB.

---

## ğŸ“… PrÃ³ximos pasos sugeridos
* Agregar un @sensor para ejecutar solo si hay nuevos datos en Bronce.
* Implementar validaciones de calidad de datos (DQ checks).
* Crear documentaciÃ³n para la capa Gold o modelado de ML.