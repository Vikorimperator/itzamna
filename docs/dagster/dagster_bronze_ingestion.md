# ðŸ§¾ DocumentaciÃ³n del pipeline de ingestiÃ³n de datos bronce con Dagster

Este documento describe la arquitectura, configuraciÃ³n y flujo de trabajo actual del pipeline de ingestiÃ³n de datos bronce utilizando **Dagster**, **DuckDB**, **Polars** y ejecuciÃ³n controlada por `@op` y `@job`.

---

## ðŸ—ï¸ Estructura general del proyecto

itzamna_pipeline/
â”œâ”€â”€ ops_ingestion.py # Contiene los ops: sensor, equipos, eventos
â”œâ”€â”€ jobs.py # Define el job secuencial de ingestiÃ³n
â”œâ”€â”€ schedules.py # Agenda el job diario a las 6am
â”œâ”€â”€ itzamna_core/
â”‚ â”œâ”€â”€ data_ingestion/
â”‚ â”‚ â””â”€â”€ ingest_bronze.py # LÃ³gica de carga CSV + Parquet + DuckDB
â”‚ â””â”€â”€ utils/
â”‚ â””â”€â”€ config.py # ConfiguraciÃ³n de rutas
â”œâ”€â”€ definitions.py # Punto de entrada registrado en Dagster

---

## ðŸ” Flujo de ejecuciÃ³n

Se orquesta la carga de datos a la base **bronce (DuckDB)** desde archivos `.csv` en `data/raw/`. El flujo es:

sensor_data â†’ equipos â†’ eventos

Esto garantiza que no haya conflictos de acceso concurrente al archivo `warehouse.duckdb`.

---

## âš™ï¸ Componentes clave

### `@op`: ingest_sensor_data

- Carga archivos `AYATSIL-*_all.csv`
- Guarda en `data/lake/bronze/sensor_data/*.parquet`
- Crea o actualiza `bronze.sensor_data` en DuckDB

### `@op`: ingest_equipos_data

- Carga archivos `BEC_AYATSIL-*_all.csv`
- Preprocesa fechas de operaciÃ³n
- Guarda en `bronze.equipos`

### `@op`: ingest_eventos_data

- Carga archivos `Eventos_AYATSIL-*_all.csv`
- Renombra y transforma fechas
- Guarda en `bronze.eventos`

---

## ðŸ§  Encadenamiento con Nothing

Para evitar paralelismo y errores de conexiÃ³n en DuckDB, se usÃ³ el patrÃ³n:

```python
@op(ins={"start": In(Nothing)})
def siguiente_op() -> Nothing:
    ...
```

Y el @job queda:

```python
@job
def ingest_all_bronze_op_based():
    sensor = ingest_sensor_data()
    equipos = ingest_equipos_data(start=sensor)
    ingest_eventos_data(start=equipos)
```

## ðŸ“… Schedule: ejecuciÃ³n diaria

En schedules.py se definiÃ³:

```python
daily_ingestion_schedule = ScheduleDefinition(
    job=ingest_all_bronze_op_based,
    cron_schedule="0 6 * * *",
    execution_timezone="America/Mexico_City",
    name="daily_ingestion_schedule"
)
```

Esto permite ejecutar el flujo diariamente (manual o automÃ¡tico si se habilita el daemon).

## ðŸ”Ž Beneficios logrados
* âœ… EjecuciÃ³n secuencial segura
* âœ… Uso de Polars + Parquet + DuckDB en modo local
* âœ… Monitoreo visual y control de errores en Dagit
* âœ… ModularizaciÃ³n para escalar a capa Silver

## ðŸ“Œ Pendientes posibles
* Agregar @sensor si se desea ejecuciÃ³n por apariciÃ³n de nuevos archivos.
* Agregar capa Silver con nuevos @op.
* Agregar pruebas unitarias a cada @op.
* Agregar una funcion para registrar automÃ¡ticamente todos los assets de un mÃ³dulo.